name: Transformer
nhead: 2    #help='the number of heads in the encoder/decoder of the transformer model')
emsize: 200     #help='size of word embeddings')
nhid: 200       #help='number of hidden units per layer')
nlayers: 2      #help='number of layers')
clip: 0.25      #help='gradient clipping')
dropout: 0.2    #help='dropout applied to layers (0 = no dropout)')
tied: True      #help='tie the word embedding and softmax weights')

lr: 20          #help='sequence length')
epochs: 40      #help='upper epoch limit')
batch_size: 20
bptt: 35        #help='initial learning rate')
