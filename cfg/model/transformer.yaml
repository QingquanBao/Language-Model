name: Transformer
# 52.06 M
nhead: 8    #help='the number of heads in the encoder/decoder of the transformer model')
emsize: 800     #help='size of word embeddings')
nhid: 800       #help='number of hidden units per layer')
nlayers: 2      #help='number of layers')
clip: 10         #help='gradient clipping')
dropout: 0.4    #help='dropout applied to layers (0 = no dropout)')
tied: True      #help='tie the word embedding and softmax weights')

lr: 0.5         #help='initial learning rate')       
epochs: 40      #help='upper epoch limit')
batch_size: 30
bptt: 50        #help='sequence length')
